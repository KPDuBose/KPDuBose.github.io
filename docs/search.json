[
  {
    "objectID": "projects/partyGames.html",
    "href": "projects/partyGames.html",
    "title": "partyGames",
    "section": "",
    "text": "partyGames\nThe partyGames package was written to simulate different games under a variety of conditions.\nCurrently, the package simulates:\n\nWhite Elephant Gift Exchanges\nMonopoly\n\nFurther information about the package, including installation instructions, can be found here.\nBrief information about functionality follows.\n\n\nWhite Elephant Games\nThe following shows how to simulate a white elephant gift game utalizing a 6-sided die and a fair coin.\n\nlibrary(partyGames)\ngames &lt;- simulateElephant(15, dice = TRUE, coins = 1, iter = 1500, pheads = 0.5, sides = 6, numDice = 1)\nsummary.partyGames(games)\n\n$winner\nwinner\n  1   2   3   4   5   6   7   8   9  10  11  12  13  14  15 \n 97 111  98  93 105 101  97  87 102 102 113 105  82 111  96 \n\n$moves\nmoves\n  -6   -5   -4   -3   -2   -1    1    2    3    4    5    6 \n1851 1835 1809 1903 1872 1942 1879 1848 1898 1876 1946 1841 \n\nattr(,\"class\")\n[1] \"elphSum\" \"list\"   \n\nclass(games)\n\n[1] \"elphList\" \"list\"    \n\nplot.partyGames(games)\n\n\n\n\n\n\n\nplot.partyGames(summary.partyGames(games))\n\n\n\nMonopoly Simulation\nAdditionally, you can simulate Monopoly games based on the 2008 US version of the game.\n\nlibrary(partyGames)\nmonopolyGame &lt;- simulateMonopoly()\nmonopolyGame\n\n[[1]]\n [1] 11  9 15 15 23  7  9 12 14 19 19 18 16 11  9 19 14 11 18 14 17 18 14 14 13\n[26] 16 15 15 14  9 15 14 17 15 14 16 13 18  8 20\n\n[[2]]\n [1] 17 12 17 12 15 17 16  6 14 14 13 17 15 12 18 10 19 14 16 20 13 12 12 13 13\n[26] 23 15 11 17 11 17 11 13 12 17 16 11  9 13 23\n\n[[3]]\n [1] 11  8 15 16 12  9 25 13 15  9 24 11 15 15 16 16 15 18 12 14 11 20 11 21 19\n[26] 10 13 14 18  9 15 14 12 21 10 17 16 13 13 14\n\n[[4]]\n [1] 12  9 19 15 11 13 19  5 18 15 18 10 17 12 14 26 11 15 14 16  9 15 20 12  9\n[26] 20 15 17 14  8 15 15 11 19 20 12 13 15 12 13\n\n[[5]]\n [1] 15  9 10 13 18 15 17 16  6 14 21 16 21 11 16 14 16 21 15 17  9 14 10 14 19\n[26] 20 13 13 16 17  7 18 15 16 17 16  9 15 14 16\n\n[[6]]\n [1] 13 10  9 19 15 13 12 13 13 16 21 19 13 13  7 21 18 20 11 15 16 14 16 11 23\n[26] 17 10 14  7 15 13 20 10 14 21 13 13 19 13 19\n\n[[7]]\n [1] 11 16 15 21  8 15 14 13 11 15 25 12 18 14 10 13 17 19 12 16 25  6 14 15 19\n[26]  5 25  8 20 16 12 16 12 14 15 10 12 22  9 13\n\n[[8]]\n [1]  9 16 11 13 14 18 11 21 11 17 20 11 15 15 14 18 13 19 15 13 16 18 14 13 13\n[26] 19 17 15 16 12 16  8 19 13 17 14 10 15 14 15\n\n[[9]]\n [1] 10 11 20 13  8 12 14 15 17 15 25 12 15 13 19 10 16 12 16 15 19  8 23 19  8\n[26] 18 20 14  5 14 14  9 12 18 21 20 12 18 12 12\n\n[[10]]\n [1] 15 15 13 18 16 16 14 12 13 15 23 18 18 13 16 11 18 10 18 14  9 17 17 15 14\n[26] 12 22 15 17 19 13  9 12 17 18 23 10 14 11 10\n\nattr(,\"class\")\n[1] \"monopoly\" \"list\"    \n\nsummary.monopoly(monopolyGame)\n\n  1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18  19  20 \n124 115 144 155 140 135 151 126 132 149 209 144 163 129 139 158 157 159 147 154 \n 21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38  39  40 \n144 142 151 147 150 160 165 136 144 130 137 134 133 159 170 157 119 158 119 155 \nattr(,\"class\")\n[1] \"monoSum\" \"numeric\"\n\nplot.monopoly(monopolyGame)\n\n\n\n\n\n\n\n\nParallel computing, through OpenMP, is also implemented as demonstrated here:\n\nmonoParallel &lt;- simulateMonopoly(numGames = 20, \n                                 maxTurns = 500, \n                                 sides    = 6, \n                                 numDice  = 2, \n                                 cores    = 3)\n\nsummary.monopoly(monoParallel)\n\n  1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18  19  20 \n248 285 259 267 293 267 298 285 265 280 440 297 256 280 289 300 311 289 310 297 \n 21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38  39  40 \n323 330 283 315 301 273 277 304 310 268 271 294 308 289 296 278 309 291 279 255 \nattr(,\"class\")\n[1] \"monoSum\" \"numeric\"\n\nplot.monopoly(monoParallel)"
  },
  {
    "objectID": "presentations/quarto-website-construction.html",
    "href": "presentations/quarto-website-construction.html",
    "title": "How to Make a Quarto Website",
    "section": "",
    "text": "First, due credit where credit is due. The following document was heavily influenced by tutorials found on the Quarto website.\nAdditionally, this tutorial does not go over how to set up a GitHub page. Please refer to the Quickstart for GitHub Pages tutorial for help getting started hosting on GitHub pages."
  },
  {
    "objectID": "presentations/quarto-website-construction.html#create-a-new-project",
    "href": "presentations/quarto-website-construction.html#create-a-new-project",
    "title": "How to Make a Quarto Website",
    "section": "Create a New Project",
    "text": "Create a New Project\nStarting to create a website is pretty easy. You simple have to create a new project in RStudio.\n\nClick on the new project button in the top left of the RStudio IDE.\nSelect new project\nSelect the “Quarto Website” option\n\n\n\n\nNew Project Wizard\n\n\n\nCreate a new name for the directory. I host my website on GitHub Pages and follow the naming convention for that (i.e. kpdubose-website which is related to kpdubose.github.io)\n\n\n\n\nName the Project\n\n\n\nThen click create project. This will create a new project with four files: _quarto.yml, about.qmd, index.qmd, and styles.css.\n\n\n\n\nThe New Project"
  },
  {
    "objectID": "presentations/quarto-website-construction.html#the-new-files",
    "href": "presentations/quarto-website-construction.html#the-new-files",
    "title": "How to Make a Quarto Website",
    "section": "The New Files",
    "text": "The New Files\nLet’s dive briefly into the new files you can see.\n\nquarto.yml\n\n\n\n_quarto.yml\n\n\nThis file is like the yml header that for most .qmd files, but this affects to the whole website. It sets the default theme, default styles, title, navbar, etc… There are a lot of features that can be played with here, that we can dive into later, or can be looked up on the Quarto website.\nYou can change the theme from page to page (kind of like I did with this page on my website) by specifying a different theme on the individual pages. (Though this is a newer feature. Older versions of Quarto will not allow the themes to differ from page to page.)\n\n\nabout.qmd\n\n\n\nabout.qmd\n\n\nThis is one of the pages on the website. If you’re using this as a portfolio website, this can be the page about you! For the example code of how I set up my “About” page, please look at my Github file.\n\n\nindex.qmd\n\n\n\nindex.qmd\n\n\nThis is the home page for the website. As with the about page, additional information can be added and changed. I have the home page set up to show a list of links to different projects and other pages on the website. Some of these are not accessible via the navbar so it can be pretty helpful. You can even just use this page as your about you page. For an example of you I set up my index page, please click here.\n\n\nstyles.css\n\n\n\nstyles.css\n\n\nThis is the styles.css page. You can assign html objects for the website here."
  },
  {
    "objectID": "presentations/quarto-website-construction.html#an-additional-file",
    "href": "presentations/quarto-website-construction.html#an-additional-file",
    "title": "How to Make a Quarto Website",
    "section": "An Additional File",
    "text": "An Additional File\n\nAdd an .scss file\nThis file isn’t generated automatically, but it can be helpful to have for changing the theme of your website.\nI like to call this file “custom.scss”. You can create this in RStudio by adding a new file and just calling it that.\nYou can use it to edit the website theme by specifying specific colors, specific fonts, background colors, etc… I edit this as needed, but most of the Quarto website themes. If you have questions about what a specific theme looks like, they can all be found here.\nI copied my main website theme from the morph theme, but then made some changes to the *.scss file associated with it. For an example of this theme, please look here."
  },
  {
    "objectID": "presentations/quarto-website-construction.html#lets-check-it-out",
    "href": "presentations/quarto-website-construction.html#lets-check-it-out",
    "title": "How to Make a Quarto Website",
    "section": "Let’s check it out",
    "text": "Let’s check it out\nThere are two important terminal commands to know when constructing your website.\nThe first is quarto preview. This is the general, let’s make sure everything is linked properly and working the way I was hoping it would.\n\n\n\nquarto preview\n\n\nThe second command is quarto render. This command builds the website. It should be run before you upload the files to the cloud, or if you make any big changes to the themes of the website.\nWhen you run the quarto preview command it should look something like this (assuming you haven’t made any changes).\n\n\n\nHome Page\n\n\n\n\n\nAbout Page\n\n\nBut yeah. That’s the basics. You can play around with it. The next few sections cover my ramblings and different work arounds that I have found for problems that I have had. These can all be found by searching the internet, but I thought it might be helpful to keep some here."
  },
  {
    "objectID": "presentations/quarto-website-construction.html#different-quarto-formats",
    "href": "presentations/quarto-website-construction.html#different-quarto-formats",
    "title": "How to Make a Quarto Website",
    "section": "Different Quarto Formats",
    "text": "Different Quarto Formats\nI’ve started to experiment with a few different formats for pages. I’m planning to keep a running list of formats I’ve gotten to integrate with the GitHub pages website properly and other tips and tricks for them.\nRenders from a .qmd with no trouble:\n\nhtml\npdf\n\nEverything tells me it should work, but it hasn’t yet:\n\nrevealjs"
  },
  {
    "objectID": "presentations/quarto-website-construction.html#hosting-on-github-pages",
    "href": "presentations/quarto-website-construction.html#hosting-on-github-pages",
    "title": "How to Make a Quarto Website",
    "section": "Hosting on GitHub pages",
    "text": "Hosting on GitHub pages\nIf you plan to host on GitHub pages, there are a few tips.\n\nThe docs folder\nIn the _quarto.yml file you can specify an output directory with the output-dir option. While I think you can build the website by specifying the output-dir to really be anything, I prefer to build my website to a folder called docs and building the website from there, which I will detail in the next section. In the meantime, add this code to your _quarto.yml file:\nproject:\n  type: website\n  output-dir: docs\nI push my entire project including all of my .qmd files and images I used in making my website, mainly for easy reference in this page of my website. When you quarto render it creates a few different files, including the “docs” directory if you have included the output-dir: docs option.\n\n\n\nFolders\n\n\nThe “docs” folder should now contain all the files to make the website. When you push to GitHub, there is an option to build from a specific branch and folder. Make sure you are building from the docs folder.\n\n\n\nPages\n\n\nIf you aren’t using git commands to push and pull from the repository, you just have to upload the new docs folder after each render.\n\n\n.nojekyll file\nIf you’re hosting on GitHub, you need to include a .nojekyll file. (For reasons better explained here)\nYou can include one by running the following line of cod in the command terminal\nFor Windows:\ncopy NUL .nojekyll\nFor Mac/Linux:\ntouch .nojekyll\n\n\nThe final push\nIn the command terminal, the following lines of code can be run to push to GitHub. This is after you’ve linked the depository to you project.\nquarto render\ngit add docs\ngit commit -m \"Publish site to docs/\"\ngit push\nAdditionally, you can just render the website and upload the docs folder to the pages website."
  },
  {
    "objectID": "presentations/quarto-website-construction.html#how-to-structure-a-_quarto.yml",
    "href": "presentations/quarto-website-construction.html#how-to-structure-a-_quarto.yml",
    "title": "How to Make a Quarto Website",
    "section": "How to structure a _quarto.yml",
    "text": "How to structure a _quarto.yml\nBelow is the code I have used for my _quarto.yml. This has a brief explanation of each code chunk and what it does for the website.\nThis section defines the project type, and the output-directory. Since I host my website on GitHub, I have my output-directory set as docs, as explained above. The render option let’s you select which documents you would like to render to the website. “*.qmd” tells Quarto to render all .qmd files. “!tournament-announcements/” and “!resumes/” tells Quarto to not render any files found in this directory\nproject:\n  type: website\n  output-dir: docs\n  render:\n    - \"*.qmd\"\n    - \"!tournament-announcements/\"\n    - \"!resumes/\"\nThe next two section deal with the look of the website. This specific section defines the title of the website as “Kline DuBose” (fitting for a portfolio website) and then establishes the navbar at the top of the page. It sets it to the left and makes then links to pages I have made and set in the navbar. There are a lot of options for this, but googling website options makes it easy for you to look into it.\nwebsite:\n  title: \"Kline DuBose\"\n  navbar:\n    left:\n      - href: index.qmd\n        text: Home\n      - href: main_about.qmd\n        text: About\n      - href: main_projects.qmd\n        text: Projects\nThis section sets up the footer for the entire website. Mine includes links with specific icons and different websites. As well as the address of my institution.\n  page-footer:\n    background: light\n    left:\n      - icon: twitter-x\n        href: https://x.com/KlineDuBose\n      - icon: github\n        href: https://github.com/KPDuBose\n      - icon: linkedin\n        href: https://www.linkedin.com/in/kline-dubose/\n      - icon: envelope\n        href: mailto:kline.dubose@utah.edu\n    right:\n      - \"295 Chipeta Way, 3rd Floor\n         Salt Lake City, UT 84108\n         United States\"\nThis last section sets additional format information for the website.\nformat:\n  html:\n    theme: \n      - morph\n      - custom.scss\n    css: styles.css\n    toc: true"
  },
  {
    "objectID": "presentations/quarto-website-construction.html#resume",
    "href": "presentations/quarto-website-construction.html#resume",
    "title": "How to Make a Quarto Website",
    "section": "Resume",
    "text": "Resume\nYou should be able to include your resume or CV on the website without too much trouble. I have a quarto document that I use to make sure it renders nicely and that I don’t make any mistakes with my poor Microsoft Word skills. Just make sure that you include a “.docx” or a ”.pdf” option in the render section of your _quarto.yml file. I haven’t played around with it a ton.\nIf you would like to see the files I’ve used to construct my CV, they can be found on my GitHub."
  },
  {
    "objectID": "presentations/quarto-website-construction.html#icons",
    "href": "presentations/quarto-website-construction.html#icons",
    "title": "How to Make a Quarto Website",
    "section": "Icons",
    "text": "Icons\nFinding the icons associated with the quarto projects. They can be found on the Quarto website under the Nav Options section, but that just redirects you to the Bootstrap icons website. Quarto uses the bootstrap framework, I think. I haven’t really delved into it, so don’t take my word for it."
  },
  {
    "objectID": "presentations/quarto-website-construction.html#section",
    "href": "presentations/quarto-website-construction.html#section",
    "title": "How to Make a Quarto Website",
    "section": "404",
    "text": "404\nYou can also include an error page. Unless you’re doing something really fancy, you can just include a 404.qmd file. My 404.qmd is pretty basic, but it fits well with my theme. Here’s what it looks like when you find it.\nIf you’re doing something fancier, check out the website navigation tutorial"
  },
  {
    "objectID": "presentations/quarto-website-construction.html#making-a-blog",
    "href": "presentations/quarto-website-construction.html#making-a-blog",
    "title": "How to Make a Quarto Website",
    "section": "Making a Blog",
    "text": "Making a Blog\nA formatting option that you can include that renders pages as an index of other pages could be helpful. For an example of this, click here for the way my website incorporates it, here too see my raw .qmd file, or here for a detailed tutorial from Quarto.\nWhen looking at the .qmd source file, we can see what the header looks like:\n---\ntitle: \"Presentations and Tutorials\"\nlisting: \n  contents: presentations\n  type: grid\n---\nLet’s break this down a little bit.\n\ntitle: just names the webpage\nlisting: tells us that we want to create a list of other pages that looks kinda pretty.\n\ncontents: tells Quarto what folder to reference here. In this case I reference all .qmd files in my presentations folder. You can also reference specific files, as seen here.\ntype: grid tells Quarto what style I want to use. Examples of other styles can be found here."
  },
  {
    "objectID": "presentations/quarto-website-construction.html#thats-it-for-now",
    "href": "presentations/quarto-website-construction.html#thats-it-for-now",
    "title": "How to Make a Quarto Website",
    "section": "That’s it, for now",
    "text": "That’s it, for now\nThat should be enough information to help you get started. Quarto is a versatile tool, and they have a lot of tutorials to help one get profecient at using it.\nFeel free to check out my GitHub project and let me know if you have any question.\nGood luck out there!"
  },
  {
    "objectID": "main_projects.html",
    "href": "main_projects.html",
    "title": "Projects",
    "section": "",
    "text": "A running list of past and current projects I can share.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npartyGames\n\n\nA Simulation Package in R\n\n\n\n\n\n\nMar 4, 2025\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "main_about.html",
    "href": "main_about.html",
    "title": "Kline DuBose",
    "section": "",
    "text": "Kline DuBose is a Graduate Research Assistant and PhD Student at the University of Utah. He has helped with data cleaning and data analysis on numerous research projects. His research interests include clinical trials, trial design, and exact tests."
  },
  {
    "objectID": "main_about.html#education",
    "href": "main_about.html#education",
    "title": "Kline DuBose",
    "section": "Education",
    "text": "Education\nUniversity of Utah | Salt Lake City, UT\nPhD in Biostatistics | Aug 2022 - Present\nUtah State University | Logan, UT\nB.S. in Statistics | Jan 2019 - May 2022"
  },
  {
    "objectID": "main_about.html#experience",
    "href": "main_about.html#experience",
    "title": "Kline DuBose",
    "section": "Experience",
    "text": "Experience\nUniversity of Utah, DELPHI | T.A. for R Coding Bootcamp | June 2023 - Present\nUniversity of Utah | Graduate Research Assistant | Aug 2022 - Present\nUtah State University | Recitation Leader, Grader | Aug 2020 - May 2022"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome",
    "section": "",
    "text": "Welcome to my website! Here you will find previous projects, current projects I can share, contact information, and more!\nIf there are any questions, comments, or concerns, please feel free to contact me.\nThe following is currently available:\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nTitle\n\n\n\n\n\n\nKline DuBose\n\n\n\n\nPresentations and Tutorials\n\n\n\n\nProjects\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "main_presentation.html",
    "href": "main_presentation.html",
    "title": "Presentations and Tutorials",
    "section": "",
    "text": "Presentations and tutorials I have done for different groups. Some of these pages are unrelated to my statistical interests, but follow other interests and can be helpful to other groups I have worked with and presented for.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow to Make a Quarto Website\n\n\n\n\n\n\nKline DuBose\n\n\nApr 22, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNeural Networks and the Bayesian Extension\n\n\n\n\n\n\nKline DuBose\n\n\nApr 25, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSpanish Cheat Sheet\n\n\n\n\n\n\nKline DuBose\n\n\nApr 3, 2025\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "presentations/dl_models_overview.html",
    "href": "presentations/dl_models_overview.html",
    "title": "Neural Networks and the Bayesian Extension",
    "section": "",
    "text": "The purpose of this write up is to summarize my study and exploration of Neural Networks (NN) and how principles of Bayesian analysis can be used and applied in this framework. The primary resource for this project was Probabilistic Deep Learning with Python, Keras, and TensorFlow Probability (Dürr, Sick, and Murina (2020)), though additional resources were used in understanding the material. Additionally, I will be using this write up to clarify and expound on information touched on in my final presentation for the class. Mainly, I hope to dive deeper into:\n\nloss functions and how they work in the NN framework.\nGradient adaptation and weights for training data.\nFlexible models and Normalizing Flow (NF)\nVariation inference (VI) and MC dropout architectures.\n\nThis write up will be structured to follow along with my final presentation and fill in the gaps that my slides and initial explanations left."
  },
  {
    "objectID": "presentations/dl_models_overview.html#nn-architectures",
    "href": "presentations/dl_models_overview.html#nn-architectures",
    "title": "Neural Networks and the Bayesian Extension",
    "section": "NN architectures",
    "text": "NN architectures\nA neural network can be compared to a regression model. A simple regression model can be written as:\n\\[\nY = \\beta_i X_i + \\varepsilon_i\n\\]\nwhere\\(Y\\)is the outcome,\\(\\beta_i\\)represents the regression parameters or slopes,\\(X_i\\)is the data, and\\(\\varepsilon_i\\)represents unmeasured noise. The comparable NN to this model looks like:\n\n\n\nSimple neural network comparable to a simple regression model\n\n\nwhere we now have our output represented by\\(p_1\\)is comparable to\\(Y\\)in our regression model,\\(x_i\\)represents the observed data just like our regression model, the arrows would represent our regression parameters, and “bias” is most comparable to the intercept or could even be thought of as the error term. It is probably best to think of it as some parameter that is unaffected by the data itself. The output exists in what is called a “neuron”. The neuron takes the data, and outputs something.\nThere were three types of networks that were really discussed by Dürr, Sick, and Murina (2020). They are:\n\nFully connected neural networks (fcNN)\nConvolutional neural networks (CNN)\nOne-dimensional CNN\n\n\nfcNN\nThe fcNN architecture connects each layer to every neuron in the next. In the picture below, we can see that each neuron in the input layer is connected to every neuron in the hidden layer. And every neuron in the hidden layer is connected to each neuron in the output layer. Each of these neurons takes information from the previous layer, and then applies weights to it. In the below picture, our two inputs would get passed to the hidden layer where each neuron has a different weight and would create two new inputs, often denoted as\\(z_i\\)but that I will denote as\\(z_{ih}\\), where\\(i\\)represents the number of neurons in the layer and\\(h\\)represents the hidden layer that it was calculated on, which are then passed onto the next layers. When they are passed to the output layer, the\\(z_{ih}\\)are converted to the final outputs, which I will call\\(z_j\\)where the\\(j\\)represents the number of classes of interest. In the fcNN we are looking at, the\\(z_{ih}\\)for each of the\\(i\\)neurons are passed to the output layer, converted to a new set of\\(z_j\\). In a classification problem, the softmax equation (\\(p_i = e^{z_i}/\\sum_j e^{z_j}\\)) takes the final output,\\(z_j\\), and converts it to a probability that each class is the correct classification. A detailed example of this principle will be shown later. As you can imagine, however, this type of model can get pretty large and complutationally intensive pretty quickly. Other models attempt to overcome this.\n\n\n\nfcNN with one hidden layer\n\n\n\n\nCNN\nThe CNN model was developed to help reduce the computational intensity of the fcNN. Instead of connecting all of the neurons to all of the neurons, it only connects some neurons to other neurons. The best way to think of it, I think, is to imagine a picture (shown below) of a dog. A fcNN would look at the full picture, which works great for our brains, but not so great for our computers. A would look at different parts of the picture and put the information together in the output layer.\n\n\n\nfcNN vs CNN\n\n\n\n\nOne-dimensional CNN\nA one-dimensional CNN behaves similarly to a CNN, but it assumes that the order of the data is important. For example, this model could be a large language model, which is used to predict the next word in this . The words leading up the are important in predicting what the word is, as changing the word can change the probability of a certain word being correct. It can be used with time data as well.\n\n\nOther networks\nThere are additional network architectures, but they tend to follow the same basic principles and were not discussed by Dürr, Sick, and Murina (2020)."
  },
  {
    "objectID": "presentations/dl_models_overview.html#tuning-the-weights-and-training-the-data",
    "href": "presentations/dl_models_overview.html#tuning-the-weights-and-training-the-data",
    "title": "Neural Networks and the Bayesian Extension",
    "section": "Tuning the weights and training the data",
    "text": "Tuning the weights and training the data\nWe can think of NN’s as an extension of regression models being used for prediction. As with other simpler models, we first give the NN a training data set with all the right answers so that it can chose the optimal weights for each of the hidden layers. How does the model know which way to adjust the weights? That depends on the loss function that you chose. The loss function can change based on the data that you are working with, but a good example is the MSE.\n\\[\nloss = MSE= \\frac 1 n \\sum_{i=1}^n (y_i - \\hat y_i)^2 = \\frac 1 n \\sum_{i=1}^n (y_i - (a \\cdot x_i +b))^2\n\\]\nIn this case, we will focus on the third part of the equation, with\\(a\\)and\\(b\\)in it. Our goal is to find the values of\\(a\\)and\\(b\\)that minimize this loss. This isn’t too hard of a problem with simple models. Here we would just solve:\n\\[\n\\begin{split}\n\\frac{\\partial loss}{\\partial a} = 0 \\\\\n\\frac{\\partial loss}{\\partial b} = 0\n\\end{split}\n\\]\nBut, this isn’t always a simple problem to solve, and the with NN, often there isn’t a simple closed form solution to this problem. Because of this, another method was developed to minimize loss when working with NN. The most common method is called the gradient descent method, which tests different points within the model until it finds the minimized loss. This can also get a little tricky, but usually works pretty well. When using this method to solve for one parameter, Dürr, Sick, and Murina (2020) describe it as a blind mathematician walking slowly into a valley until he finds the lowest point. In the case of the loss function previously written and attempting to minimize for\\(a\\), we can use the following update rule:\n\\[\na_{t+1} = a_t - \\eta \\cdot grad_a (loss)\n\\]\nwhere\\(a_{t+1}\\)is the updated parameter,\\(a_t\\)is the last parameter we’d calculated under our rule,\\(grad_a\\)is the gradiant of the loss function with respect to\\(a\\), and\\(\\eta\\)is our learning rate and controls how fast we jump around trying to find the minimized loss.\nWe can then extend this idea to estimating both parameters\\(a\\)and\\(b\\). This would result in an update formula:\n\\[\n\\begin{split}\n  a_{t+1} & = a_t - \\eta \\cdot grad_a \\\\\n  b_{t+1} & = b_t - \\eta \\cdot grad_b\n\\end{split}\n\\]\nWhen we treat\\(b\\)as fixed, we can find the gradient as:\n\\[\ngrad_a = \\frac{\\partial}{\\partial a} \\frac 1 n \\sum_{i=1}^2 (a \\cdot x_i + b - y_i)^2 = \\frac 2 n \\sum_{i=1}^n (a \\cdot x_i + b - y_i) \\cdot x_i\n\\]\nWe can do the same by keeping\\(a\\)fixed. If we’re doing that, we can find the gradient for\\(b\\)to be:\n\\[\ngrad_b = \\frac{\\partial}{\\partial b} \\frac 1 n \\sum_{i=1}^2 (a \\cdot x_i + b - y_i)^2 = \\frac 2 n \\sum_{i=1}^n (a \\cdot x_i + b - y_i)\n\\]\nWe plug these values into the gradient rule functions we explained earlier, and viola, we start to find the minimum values for our lose functions.\nIt should be noted here that our choice of\\(\\eta\\)is important. If we choose something too big, there’s a chance we never find the optimal values of\\(a\\)and\\(b\\), but with an\\(\\eta\\)that is to small, we need many more update steps. I would argue that it is better to have a too small learning parameter than a too large one, though this would obviously require more data. (Additionally, Dürr, Sick, and Murina (2020) state that if the loss starts to get too big, a good rule of thumb is to divide your current learning rate by 10 and see what happens.)\nWhat we have done so far is pretty commonly done in simple linear regression. There are some additional tricks we can apply.\n\nMini-batch gradient descent or Stochastic Gradient Descent (SGD)\nThis process involves randomly selecting data points from the data, which is sometimes called a mini-batch, to get an unbiased estimator of the data. Often, GPUs have limited memory so this helps resolve that problem. It can happen that we don’t find the global minimum, but for some reason that is not completely understood, this procedure works pretty well whe fitting DL models.\n\n\nSGD variants that speed up the learning\nThere are two prominent algorithms, RMSProb and Adam, that are included in most DL framework. The methods aren’t all that different from SGD, but often speed up the process. Dürr, Sick, and Murina (2020) doesn’t delve into the differences between all these methods, but Goh (2017) provides a good overview of SGD and other methods previously mentioned. I still haven’t fully understood the difference between RMSProb and Adam, but after skimming the article by Goh (2017), I think it has to do with the addition of a diagonal matrix,\\(\\Gamma_i^k\\), which allows different step sizes for different directions. Our update formulas would look like this (first in the notation of Goh (2017) and then an attempt to translate it to our notation):\n\\[\nw^{k+1} = w^0 + \\sum_i^k \\Gamma_i^k \\nabla f(w^i) \\Rightarrow a_{t+1} = a_t - \\sum_i^k \\Gamma_i^k grad_a\n\\]\nIf I am understanding this correctly,\\(\\Gamma_i^k\\)takes the place of\\(\\eta\\)and we changes somehow based of the gradient we are looking at. I want to look more into this, but have started to run out of time.\n\n\nAutomatic differentiation\nBasically, we use the chain rule,\\(\\left( f(g(x)) \\right)' = f'(g(x)) \\cdot g'(x)\\), to update our parameter values with the update rule. This is were the Backpropagation, or reverse-mode differentiation, comes into play. Here, we basically start with the final loss value and move from our outer layer to our input layer to get our gradient with respect to each of our model parameters."
  },
  {
    "objectID": "presentations/dl_models_overview.html#an-example-with-the-help-of-chatgpt",
    "href": "presentations/dl_models_overview.html#an-example-with-the-help-of-chatgpt",
    "title": "Neural Networks and the Bayesian Extension",
    "section": "An example, with the help of ChatGPT",
    "text": "An example, with the help of ChatGPT\nAfter my presentation, I was thinking through the role of the loss function, backpropogation, the gradient, etc… and I was having a hard time understanding the concept. Once I got better at thinking it through, I talked to ChatGPT and it helped create a simple scenario to illustrate how the loss function and a basic NN would work.\nLet’s set up the following scenario. Let’s pretend to have set up a basic NN with the purpose of deciding whether or not some one gets a lone. We have two inputs (\\(x_1 = 4.0\\),\\(x_2 = 0.82\\)) which represent income and credit score (divided by 100). Our hidden layer has 4 neurons that use a Rectified Linear Unit (ReLU) that was suggest by ChatGPT. ReLU translates input values so that if the input is positive, it stays the same, but if the input is zero or negative it becomes 0.\n\\[\nReLU(x) = \\begin{cases}\nx & \\text{if } x &gt; 0 \\\\\n0 & \\text{if } x \\leq 0\n\\end{cases}\n\\]\nFinally, we have 3 neurons on our output layer: approved, Needs Review, and Denied. We assume this is a fcNN\nOur NN looks like this:\n\n\n\nExample of a Neural Network to decide loan approval\n\n\nOur hypothetical person should have been denied. Right now, our hidden layers have the following weights and bias:\n\n\n\nNeuron\n\\(w_1\\)\n\\(w_2\\)\n\\(b\\)\n\n\n\n\n\\(h_1\\)\n0.5\n0.3\n-0.4\n\n\n\\(h_2\\)\n-0.2\n0.8\n0.1\n\n\n\\(h_3\\)\n0.1\n-0.5\n0.2\n\n\n\\(h_4\\)\n0.7\n0.6\n-0.3\n\n\n\nWe pass\\(x_1\\)and\\(x_2\\)to the hidden layers and get our\\(z\\).\n\\[\n\\begin{split}\nh_1: \\; z = 0.5(4.0)+0.3(0.82)−0.4 & =2.0+0.246−0.4=1.846 \\\\\nReLU(1.846) & = 1.846 \\\\\nh_2: \\; z  = −0.2(4.0)+0.8(0.82)+0.1 & =−0.8+0.656+0.1=−0.044 \\\\\nReLU(−0.044) & = 0 \\\\\nh_3: \\; z  =0.1(4.0)−0.5(0.82)+0.2 & =0.4−0.41+0.2=0.19 \\\\\nReLU(0.19) & =0.19 \\\\\nh_4: \\; z =0.7(4.0)+0.6(0.82)−0.3 & =2.8+0.492−0.3=2.992 \\\\\nReLU(2.992) & =2.992\n\\end{split}\n\\]\nOur output from the hidden layer is:\n\\[\nz_h = [1.846, 0, 0.19, 2.992]\n\\]\nWe pass this to the Output Layer, which will have four weights, one for each part of\\(z_h\\).\n\n\n\n\n\n\n\n\n\n\n\nOutput Neuron (Class)\n\\(w_1\\)\n\\(w_2\\)\n\\(w_3\\)\n\\(w_4\\)\nBias \\(b\\)\n\n\n\n\nApproved\n0.6\n-0.1\n0.3\n0.8\n0.2\n\n\nNeeds Review\n-0.3\n0.5\n0.1\n0.4\n-0.1\n\n\nDenied\n0.2\n-0.4\n-0.5\n0.1\n0.0\n\n\n\nIn much the same way as with the hidden layer, we can calculate our raw output as logit to get our final output \\(z\\) as:\n\\[\nz = [3.7582, 0.562, 0.5734]\n\\]\nWe exponentiate \\(z\\) and apply the softmax formula to calculate the probabilities.\n\\[\ne^z = [42.89, 1.754, 1.774]\n\\]\nWe note that the \\(sum(e^z) = 46.418\\) and get the following probabilities:\n\\[\np_{loan} = \\left[\\frac{42.89}{46.418}=0.924, \\frac{1.754}{46.418}=0.038, \\frac{1.774}{46.418}=0.038 \\right]\n\\]\nwhich represent the probabilities associated with the new subject being approved, needing further review, or being denied.\nOur model as is predicts that this new subject should be approved with 92.4% confidence. That’s some pretty big confidence just to be wrong.\nNot to worry though, we are still training the data. We will use the categorical cross-entropy (suggested by ChatGPT) to calculate loss.\n\\[\nloss = -\\sum_i y_i \\log(\\hat y_i)\n\\]\nWe take the probability of the correct class (being denied):\n\\[\n-\\log(0.038) = 3.27\n\\]\nWe can now calculate the gradient of the loss with respect to each output logit:\n\\[\n\\begin{split}\n\\frac{\\partial loss}{\\partial z} & = \\hat y_i - y_i \\\\\n& = [0.924 - 0, 0.038 - 0, 0.038 - 1] \\\\\n& = [0.924, 0.038, -0.96s]\n\\end{split}\n\\]\nWe pick the learning rate \\(\\eta = 0.1\\). With this, we go from the output layer to our hidden layer (backpropigation in action) using a variation of the formula that we have previously defined (and treating \\(x_i\\) as a vector) we get a whole new set of weights:\n\\[\nw_{approved} = w_{ij} - \\eta \\frac{\\partial loss}{\\partial z} \\cdot h_i\n\\]\nWhen we use this formula, we get a list of weights to be used for \\(z_h\\) passed from the hidden layers. This gives us the new weights with the bias remaining the same:\n\n\n\n\n\n\n\n\n\n\n\nOutput Neuron (Class)\n\\(w_1\\)\n\\(w_2\\)\n\\(w_3\\)\n\\(w_4\\)\nBias \\(b\\)\n\n\n\n\nApproved\n0.429\n-0.1\n0.282\n0.524\n0.108\n\n\nNeeds Review\n-0.3\n0.5\n0.1\n0.4\n-0.1\n\n\nDenied\n0.378\n-0.4\n-0.4817\n0.388\n0.0962\n\n\n\nIf we rerun this without changing anything with the hidden layer, we would get:\n\\[\nz = [2.52, 0.562, 1.8627]\n\\]\nAnd after using the softmax formula:\n\\[\np_{loan} = [0.602, 0.085, 0.313]\n\\]\nWhich still favors approving the individual, but has greatly decreased the probability of them being approved for a loan. If we were to move forward with this training, the predictions would get better and better.\n\n\n\nClass\nBefore\nAfter\nChange\n\n\n\n\nApproved\n92.4%\n60.2%\nDecrease\n\n\nNeeds Review\n3.8%\n8.5%\nIncrease\n\n\nDenied\n3.8%\n31.3%\nIncrease\n\n\n\nIt should be noted that this isn’t a perfect example, but should help in understanding a simplified version of the algorithm that is being applied."
  },
  {
    "objectID": "presentations/dl_models_overview.html#the-likelihood-approach",
    "href": "presentations/dl_models_overview.html#the-likelihood-approach",
    "title": "Neural Networks and the Bayesian Extension",
    "section": "The Likelihood Approach",
    "text": "The Likelihood Approach\nOne of the questions that remains is how do we find the loss function? Traditional NN and DL models us the maximum likelihood approach. The loss function that is chosen can vary and depends on the type of question we are trying to answer. Let’s use the example of a six-sided die with a $-sign replacing some of the sides. Let’s say we want to use an NN that predicts the number of dollar signs on the dice if we know that we got exactly two dollar signs in exactly ten throws.\nWe would take the equation:\n\\[\nP(\\text{training}) = \\Pi_{\\text{j with }y=0} p_0(x_j) \\Pi_{\\text{j with }y = 1} p_1(x_j)\n\\]\nand tune the weights of our model to maximize this equation. You might recognize this as a classic classification problem. Our next step is take the log of the function:\n\\[\n\\log(P( \\text{training} )) = \\sum_{\\text{j with }y=0} \\log p_0(x_j) +  \\sum_{\\text{j with }y = 1} \\log p_1(x_j)\n\\]\nFinally, we need to make the jump from our maximum log likelihood to our crossentropy formula. We do this by making the log-likelihood scale independent (by multiplying by \\(1/n\\)) and then getting the minimum value. This leaves us with a crossentropy equation of:\n\\[\n\\text{crossentropy} = - \\frac 1 n \\left( \\sum_{\\text{j with }y=0} \\log p_0(x_j) +  \\sum_{\\text{j with }y = 1} \\log p_1(x_j)  \\right)\n\\]\nThis equation works for two classification classes, but it can be extended to work for a multinomial distribution. :\n\\[\nP(Y = k|x,w) = \\left\\{\\begin{aligned}\n  p_0(x,w) \\; for \\; k=0 \\\\\n  p_1(x,w) \\; for \\; k=1 \\\\\n  \\vdots \\\\\n  p_9(x,w) \\; for \\; k=9\n\\end{aligned} \\right. with \\; \\sum_{i=0}^9 p_i(x,w) = 1\n\\]\nOften in classification problems, a variation of this is used called the Kullback-Leibler Divergence:\n\\[\n\\text{crossentropy} = - \\frac 1 n \\sum_{i=1}^n p_i^{\\text{true}} \\cdot \\log(p_i)\n\\]\nwhere \\(p_i^{\\text{true}}\\) is 1 for the true class of the training example and 0 for the other components. (To see this in action, see the loan example.)"
  },
  {
    "objectID": "presentations/dl_models_overview.html#flexible-probability-distributions-and-normalizng-flows",
    "href": "presentations/dl_models_overview.html#flexible-probability-distributions-and-normalizng-flows",
    "title": "Neural Networks and the Bayesian Extension",
    "section": "Flexible Probability Distributions and Normalizng Flows",
    "text": "Flexible Probability Distributions and Normalizng Flows\nWe are now getting to the section of the book that started to mess with my brain the most. In this section we will discuss to additional and important extensions for NN:\n\nFlexible probability distributions\nNormalizing flows (NF)\n\n\nFlexible probability distributions\nReal world data can start to get kind of messy. Because of this, mixture distributions have been used to make models more flexible. Often times this models are autoregressive, since they assume the the distribution can be estimated using previous information.\nTake for example WaveNet by Google. Additional information about WaveNet can be found on Google’s text-to-speech website. To summarize this model very briefly, Google wanted to generate realistic sounding voices for text-to-speech. They used a specialized version of CNN called a dilated convolution. Say we have some raw audio, for each time \\(t\\) we discretize the sound data. (Dürr, Sick, and Murina (2020) say that 16-bit is used for this, but I’m not sure what this means yet.) This means that at time \\(t\\), our audio signal \\(x_t\\) takes on a discrete value from \\(0\\) to \\(2^{16} - 1 = 65,535\\). In WaveNet, they then assumed that \\(x_t\\) depends on the audio signal samples earlier in time, making this a one-dimensional CNN. This means that:\n\\[\nP(x_t) = P(x_t|x_{t-1}, x_{t-2}, ... , x_0)\n\\]\nso we can sample values of \\(x_t\\) from previous values. This is called an autoregressive model, meaning that our input depends on the previous observations and inputs. The WaveNet people essentially added a start sequence of audio values to the trained WaveNet and then predicted \\(P(x_t)\\) based on the start values provided.\nAnother example of a flexible autoregressive model is PixelCNN, which was created by OpenAI and applies the principles already discussed, but to a picture instead. This means that we predict a pixel color based on the past pixel colors we have observed.\nThese models created mixture distributions, specifically discretized logistic mixture models. They took a logistic distribution, gave it limits that it could work in (i.e. \\(0\\) to \\(2^16 - 1\\)) and quantized it.\n\n\n\nDiscretized Logistic Models (Figure 6.4 in Dürr, Sick, and Murina (2020))\n\n\nThen, the mixed a couple of distributions together, as seen below:\n\n\n\nDiscretized Logistic Mixture Models (Figure 6.5 in Dürr, Sick, and Murina (2020))\n\n\nBy creating a mixture of simple base distributions, we have created a flexible way to model much more complex distributions. This works great in low-dimensional spaces, but what can we do if we want to work in higher-dimensional space?\n\n\nNormalizing flows (NFs)\nWhat is an NF? Honestly, I still don’t get it 100%, but I’m starting to get there. “IN a nutshell, an NF learns a transformation (flow) from a simple high-dimensional distribution to a complex one.” (Dürr, Sick, and Murina (2020)) Which sound really impressive, but still isn’t the most clear. The goal of an NF is to fit a complex distribution without picking a distribution in advanced or setting up a mixutre of several distributions. These are used in facial image generation, and we can draw or pick a new facial image from this complex distribution. How does this work? Basically, an NF using Jacobian transformations to get from a simple distribution, \\(p_z(z)\\) to the complex distribution, \\(p_x(x)\\). This can be direct say by going from \\(z \\rightarrow x\\) or more complex by going from \\(z \\rightarrow y \\rightarrow ... \\rightarrow x\\). The paper “Density Estimation using Real NVP” by Dinh, Sohl-Dickstein, and Bengio (2016) provides a great introduction to these types of models and some additional extensions that have been made. Though, I won’t lie, I haven’t yet had a chance to read and understand this article.\nFlow models are pretty good at playing with images. For example, see the images below:\n\n\nThey basically take a face, and within similar looking faces, can change the image little by little and extrapolate information based on these models. The normalizing flow takes the simple input and in the more complex x-distribution, models that would need to change and outputs the x distribution.\nThere’s a lot of cool stuff going on in the traditional DL models."
  },
  {
    "objectID": "presentations/dl_models_overview.html#the-coin-toss-example",
    "href": "presentations/dl_models_overview.html#the-coin-toss-example",
    "title": "Neural Networks and the Bayesian Extension",
    "section": "The Coin Toss Example",
    "text": "The Coin Toss Example\nLet’s say we are making an NN that predicts the next value of a coin. We assume that the coin is fair, but we start using our flips as the training data. We flip the coin twice and the outcome is heads both times.\nUsing the traditional maximum likelihood approach, based on the data we have, the NN would predict that the third toss would be heads as well. We can look at this a little in the graphs below:\n\n\n\nCoin Toss uning maximum likelihood\n\n\nSince all of our data says that the coin only lands on head, our maximum likelihood based model also predicts with 100% certainty that the third coin toss will result in heads.\nWhen we use the Bayesian approach, however, we can use a prior distribution to get an added estimate of our uncertainty. In this same case, we assume the coin to be fair (having the same probability of landing heads or tails) and use the appropriate prior predictive distribution. Let’s look at the same graphs under this approach and see how things change:\n\n\n\nBayesian NN Coin Flip\n\n\nWe can see in the posterior predictive distribution that our model would also predict the coin would land on heads, but unlike the previous approach, there is some uncertainty that is involved. The BNN would predict the next toss to be heads with an 80% chance.\nAs we get more and more data, our model becomes less and less dependent on the prior information except in areas where the data may be sparse."
  },
  {
    "objectID": "presentations/dl_models_overview.html#bayesian-neural-networks-bnn-architecture",
    "href": "presentations/dl_models_overview.html#bayesian-neural-networks-bnn-architecture",
    "title": "Neural Networks and the Bayesian Extension",
    "section": "Bayesian Neural Networks (BNN) Architecture",
    "text": "Bayesian Neural Networks (BNN) Architecture\nOne of the main differences between BNN and the traditional NN is that the weights of the different parts of the model are replaced by a distribution, but how can we approximate these distributions in a timely manner that won’t make our computer blow up?\n\n\n\nExample BNN from Dürr, Sick, and Murina (2020)\n\n\nThere are two different BNN methods that were discussed in Dürr, Sick, and Murina (2020):\n\nVariation inference (VI)\nMonte Carlo dropout (MC dropout)\n\nWe will discuss briefly how these two methods work and how they differ from each other.\n\nVariation inference (VI)\nThe main idea behind VI is that we approximate the weight distributions with something called a variational distribution. The default in most DL software (like Tensor Flow Probability(TFP)) is the Gaussian variational distribution. These means that the model has to learn two weight distributions, the mean \\(w_\\mu\\) and variance \\(w_\\sigma\\). For the prior, the standard normal \\(N(0,1)\\) is commonly used. As more information is gathered, the weights for the variational distributions are updated. This process minimizes the difference between our posterior distribution and the true distribution of the weights that we need. Often, this is done using the Kullback-Leibler distribution.\n\n\nMonte Carlo dropout (MC droupout)\nThis is the other method used to create BNN. It should be noted that BNNs have twice as many parameters since we are estimating a mean and a standard deviation. This can be kind of taxing on a computer, so we randomly chose neurons to be set to zero. This was demonstrated in 2014 to prevent overfitting by Srivastava et al. (2014) in traditional NNs. This idea was later applied to BNNs by Gal and Ghahramani (2016), and it worked pretty well. We can get an idea of how this works by looking at the image below:\n\n\n\na) Full model with all neurons. b) and c) show two different models that have had random neurons sent to 0 and thinned\n\n\nMC dropout is widely used in training data. This isn’t the only way to use it, however, as we can also use it in testing datasets.\nHow does this work? Gal and Ghahramani (2016) used a framework similar to VI. (They had an additional regulation parameter, but that is commonly removed in practice, according to Dürr, Sick, and Murina (2020)). The main difference between the MC dropout and VI methods is the addition of the tuning parameter \\(p^*\\), which represents the proportion of neurons to be set to \\(0\\) in the training and testing stages of the model creation. If the model isn’t behaving how you want it to, you can simply adjust \\(p^*\\) and try again.\nOne final thing I would like to not about BNN is that we can get a predictive distribution by averaging over the weight distributions of the node. This distribuiton looks like:\n\\[\np(y| x_{test}, D) = \\sum_i p(y|x_{text}, w_i) \\cdot p(w_i|D)\n\\]\nwhere \\(x_{test}\\) represents the test inputs, \\(w_i\\) represents the weight parameters at the different neurons, and \\(D\\) represents the data that you used to train the model. By using the same test inputs, \\(x_{test}\\), many times over, \\(T\\), and passing the input through the dropout model, we can combine our dropout predictions to get the Bayesian predictive distribution:\n\\[\np(y|x_{test}, D) = \\frac 1 T \\sum_{t=1}^T p(y|x_{test}, w_t)\n\\]\nThis final predictive distribution captures our uncertainty. Dürr, Sick, and Murina (2020) describes this uncertainty as “epistemic and … aleatoric”, which means it covers our lack of knowledge and chance imbalances."
  },
  {
    "objectID": "presentations/spanish-class-nov2024.html",
    "href": "presentations/spanish-class-nov2024.html",
    "title": "Spanish Cheat Sheet",
    "section": "",
    "text": "The following document covers basic grammar, important phrases, and songs to help you learn some basic Spanish."
  },
  {
    "objectID": "presentations/spanish-class-nov2024.html#ar",
    "href": "presentations/spanish-class-nov2024.html#ar",
    "title": "Spanish Cheat Sheet",
    "section": "-ar",
    "text": "-ar\n\nConjugation of “-ar” verbs with “Hablar” (To speak)\n\n\nPronoun\nEnglish\nVerb\nExample\n\n\n\n\nYo\nI\n-o\nHablo\n\n\nTú\nYou (Informal)\n-as\nHablas\n\n\nÉl/Ella/Usted\nHe/She/You (Formal)\n-a\nHabla\n\n\nNosotros/-as\nWe\n-amos\nHablamos\n\n\nEllos/-as/Ustedes\nThey/Y’all\n-an\nHablan"
  },
  {
    "objectID": "presentations/spanish-class-nov2024.html#er",
    "href": "presentations/spanish-class-nov2024.html#er",
    "title": "Spanish Cheat Sheet",
    "section": "-er",
    "text": "-er\n\nConjugation of “-er” verbs with “Aprender” (To learn)\n\n\nPronoun\nEnglish\nVerb\nExample\n\n\n\n\nYo\nI\n-o\nAprendo\n\n\nTú\nYou (Informal)\n-es\nAprendes\n\n\nÉl/Ella/Usted\nHe/She/You (Formal)\n-e\nAprende\n\n\nNosotros/-as\nWe\n-emos\nAprendemos\n\n\nEllos/-as/Ustedes\nThey/Y’all\n-en\nAprenden"
  },
  {
    "objectID": "presentations/spanish-class-nov2024.html#ir",
    "href": "presentations/spanish-class-nov2024.html#ir",
    "title": "Spanish Cheat Sheet",
    "section": "-ir",
    "text": "-ir\n\nConjugation of “-ir” verbs with “Vivir” (To live)\n\n\nPronoun\nEnglish\nVerb\nExample\n\n\n\n\nYo\nI\n-o\nVivo\n\n\nTú\nYou (Informal)\n-es\nVives\n\n\nÉl/Ella/Usted\nHe/She/You (Formal)\n-e\nVive\n\n\nNosotros/-as\nWe\n-imos\nVivimos\n\n\nEllos/-as/Ustedes\nThey/Y’all\n-en\nViven"
  },
  {
    "objectID": "presentations/spanish-class-nov2024.html#con-mi-burito-sabanero",
    "href": "presentations/spanish-class-nov2024.html#con-mi-burito-sabanero",
    "title": "Spanish Cheat Sheet",
    "section": "Con mi burito sabanero",
    "text": "Con mi burito sabanero\n\n\nRepeated three times\n\n\n\n\n\n\nSpanish\nEnglish\n\n\n\n\nCon mi burrito sabanero, voy camino de Belén\nWith my little donkey, I’m going to Bethlehem\n\n\nCon mi burrito sabanero, voy camino de Belén\n…\n\n\nSi me ven, si me ven\nIf you see me, if you see me\n\n\nVoy camino de Belén\nI’m going to Bethlehem\n\n\nSi me ven, si me ven\n…\n\n\nVoy camino de Belén\n…\n\n\nEl lucerito mañanero ilumina mi sendero\nThe morning light (star) illuminates my path\n\n\nEl lucerito mañanero ilumina mi sendero\n…\n\n\nSi me ven, si me ven\n…\n\n\nVoy camino de Belén\n…\n\n\nSi me ven, si me ven\n…\n\n\nVoy camino de Belén\n…\n\n\nCon mi cuatrico voy cantando, mi burrito va trotando\nWith my guitar, I sing while my little donkey trots\n\n\nCon mi cuatrico voy cantando, mi burrito va trotando\n..\n\n\nSi me ven, si me ven\n…\n\n\nVoy camino de Belén\n…\n\n\nSi me ven, si me ven\n…\n\n\nVoy camino de Belén\n…\n\n\nTuki tuki tuki tuki\nNo meaning, just fun to say\n\n\nTuki tuki tuki ta\n…\n\n\nApúrate, mi burrito\nHurry up, my little donkey\n\n\nQue ya vamos a llegar\nWe’re about to get there\n\n\nTuki tuki tuki tuki\n…\n\n\nTuki tuki tuki tu\n…\n\n\nApúrate mi burrito\nHurry up, my little donkey\n\n\nVamos a ver a Jesús\nWe are going to see Jesus"
  },
  {
    "objectID": "presentations/spanish-class-nov2024.html#el-conejito-de-jesús",
    "href": "presentations/spanish-class-nov2024.html#el-conejito-de-jesús",
    "title": "Spanish Cheat Sheet",
    "section": "El conejito de Jesús",
    "text": "El conejito de Jesús\n\nJesús, Jesús, quiero contarte una cosa\n(A ver, ¿qué es?, dímelo, niña preciosa)\nTengo un conejito negro con su cola corta y sus ojos café\nDice mi tío Camilo, que lo halló jugando allá por el petén\nYa ves, Jesús, ¿acaso te descuidaste?\nDice mamá que los conejos son tuyos (aja)\nVieras cómo se divierte jugando conmigo allá por el jardín (qué bien)\nDime, Jesusito lindo, que me lo has prestado para ser feliz\nSí, yo te lo presté, para que juegues con él\nNo es conejo de peluche como algunos niños tienen por allí\nEste lo hice el otro día y te lo he mandado porque crees en mí\nJesús, Jesús, quiero contarte una cosa\n(A ver, ¿qué es?, dímelo, niña preciosa)\nTengo un conejito negro con su cola corta y sus ojos café\nDice mi tío Camilo, que lo halló jugando allá por el petén\n(A ver, cuéntame más)\nYa ves, Jesús, ¿acaso te descuidaste?\nDice mamá que los conejos son tuyos\nVieras cómo se divierte jugando conmigo allá por el jardín\nDime, Jesusito lindo, que me lo has prestado para ser feliz\nSí, yo te lo presté, para que juegues con él\nNo es conejo de peluche como algunos niños tienen por allí\nEste lo hice el otro día y te lo he mandado porque crees en mí\nSí, yo te lo presté, para que juegues con él\nNo es conejo de peluche como algunos niños tienen por allí\nEste lo hice el otro día y te lo he mandado porque crees en mí"
  },
  {
    "objectID": "presentations/spanish-class-nov2024.html#la-macarena",
    "href": "presentations/spanish-class-nov2024.html#la-macarena",
    "title": "Spanish Cheat Sheet",
    "section": "La macarena",
    "text": "La macarena\n\nMostly English\n\n\n\nSpanish (OG)\n\nDale a tu cuerpo alegría Macarena\nQue tu cuerpo es pa’ darle alegría y cosa buena\nDale a tu cuerpo alegría, Macarena\nHey Macarena, ay\nDale a tu cuerpo alegría Macarena\nQue tu cuerpo es pa’ darle alegría y cosa buena\nDale a tu cuerpo alegría, Macarena\nHey Macarena, ay\nMacarena tiene un novio que se llama\nQue se llama de apellido Vitorino\nY en la jura de bandera del muchacho\nSe la dio con dos amigos, ay\nMacarena tiene un novio que se llama\nQue se llama de apellido Vitorino\nY en la jura de bandera del muchacho\nSe la dio con dos amigos, ay\nDale a tu cuerpo alegría Macarena\nQue tu cuerpo es pa’ darle alegría y cosa buena\nDale a tu cuerpo alegría, Macarena\nHey Macarena, ay\nDale a tu cuerpo alegría Macarena\nQue tu cuerpo es pa’ darle alegría y cosa buena\nDale a tu cuerpo alegría, Macarena\nHey Macarena, ay\nMacarena, Macarena, Macarena\nQue te gustan los veranos de Marbella\nMacarena, Macarena, Macarena\nQue te gusta la movida guerrillera, ay\nAy\nDale a tu cuerpo alegría Macarena\nQue tu cuerpo es pa’ darle alegría y cosa buena\nDale a tu cuerpo alegría, Macarena\nHey Macarena, ay\nDale a tu cuerpo alegría Macarena\nQue tu cuerpo es pa’ darle alegría y cosa buena\nDale a tu cuerpo alegría, Macarena\nHey Macarena, ay\nMacarena sueña con el corte inglés\nY se compra los modelos más modernos\nLe gustaría vivir en Nueva York\nY ligar un novio nuevo, ay\nMacarena sueña con el corte inglés\nY se compra los modelos más modernos\nLe gustaría vivir en Nueva York\nY ligar un novio nuevo, ay\nDale a tu cuerpo alegría Macarena\nQue tu cuerpo es pa’ darle alegría y cosa buena\nDale a tu cuerpo alegría, Macarena\nHey Macarena, ay\nDale a tu cuerpo alegría Macarena\nQue tu cuerpo es pa’ darle alegría y cosa buena\nDale a tu cuerpo alegría, Macarena\nHey Macarena, ay\nMacarena tiene un novio que se llama\nQue se llama de apellido Vitorino\nY en la jura de bandera del muchacho\nSe la dio con dos amigos, ay\nMacarena tiene un novio que se llama\nQue se llama de apellido Vitorino\nY en la jura de bandera del muchacho\nSe la dio con dos amigos, ay\nDale a tu cuerpo alegría Macarena\nQue tu cuerpo es pa’ darle alegría y cosa buena\nDale a tu cuerpo alegría, Macarena\nHey Macarena, ay\nDale a tu cuerpo alegría Macarena\nQue tu cuerpo es pa’ darle alegría y cosa buena\nDale a tu cuerpo alegría, Macarena\nHey Macarena, ay\nAy\nDale a tu cuerpo alegría Macarena\nQue tu cuerpo es pa’ darle alegría y cosa buena\nDale a tu cuerpo alegría, Macarena\nHey Macarena, ay\nDale a tu cuerpo alegría Macarena\nQue tu cuerpo es pa’ darle alegría y cosa buena\nDale a tu cuerpo alegría, Macarena\nHey Macarena, ay\nDale a tu cuerpo alegría Macarena\nQue tu cuerpo es pa’ darle alegría y cosa buena\nDale a tu cuerpo alegría, Macarena\nHey Macarena, ay\nDale a tu cuerpo alegría Macarena\nQue tu cuerpo es pa’ darle alegría y cosa buena\nDale a tu cuerpo alegría, Macarena\nHey Macarena, ay\nDale a tu cuerpo alegría Macarena\nQue tu cuerpo es pa’ darle alegría y cosa buena"
  }
]